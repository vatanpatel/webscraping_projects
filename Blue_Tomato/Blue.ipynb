{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We import all the necessary libraries\n",
    "\n",
    "from selenium import webdriver\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We define our driver. We are working with selenium which needs chromedriver. \n",
    "## You can download it from https://chromedriver.chromium.org/downloads\n",
    "## If you are using some other browser, you can download it's driver from a simple search on google.com\n",
    "## We define our driver and set the executable_path to the path of out chromedriver.\n",
    "\n",
    "bw = webdriver.Chrome(executable_path='/Users/Vatanpatel/Documents/greendeck/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We have 33 pages. The first 32 pges have 96 products each. The last page has 69 products. We will use css selectors\n",
    "## to get the product url from each page. Then we will open this url and. This opens the product page.\n",
    "## Now we use css selectors for price, product url, title, brand and image url and savee the information in our empty lists.\n",
    "## We run it for all the products on all the pages. If any value is not available, we impute NAN into the list.\n",
    "\n",
    "price = []\n",
    "prod_url = []\n",
    "title = []\n",
    "img_url = []\n",
    "brand = []\n",
    "for j in np.arange(33)[1:]:\n",
    "    bw.get('https://www.blue-tomato.com/de-AT/products/categories/Snowboard+Shop-00000000/?page={}'.format(j))\n",
    "    for i in np.arange(96):          \n",
    "            url = bw.find_element_by_css_selector('#p{} > span.productdesc > a'.format(i)).get_attribute('href')\n",
    "            bw.get(url)\n",
    "            prod_url.append(url)\n",
    "            try:\n",
    "                price.append(bw.find_element_by_css_selector('.c-details-box__price-current').text)\n",
    "            except:\n",
    "                price.append('NAN')\n",
    "            try:\n",
    "                brand.append(bw.find_element_by_css_selector('.c-details-box__name > span:nth-child(1)').text)\n",
    "            except:\n",
    "                brand.append('NAN')\n",
    "            try:\n",
    "                title.append(bw.find_element_by_css_selector('#variantName').text)\n",
    "            except:\n",
    "                title.append('NAN')\n",
    "            try:\n",
    "                img_url.append(bw.find_element_by_css_selector('.js-media-box-img').get_attribute('src'))\n",
    "            except:\n",
    "                img_url.append('NAN')\n",
    "            finally:\n",
    "                bw.get('https://www.blue-tomato.com/de-AT/products/categories/Snowboard+Shop-00000000/?page={}'.format(j))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The script stopped working at page 12 product 25. We create a dataframe for our results till page 12.\n",
    "## We also removed the last row as it had missing values.\n",
    "\n",
    "blue = pd.DataFrame(title, price)\n",
    "blue = blue.reset_index()\n",
    "blue.columns = ['Price', 'Name']\n",
    "blue['Image_URL'] = img_url\n",
    "blue['Product_URL'] = prod_url\n",
    "blue['Brand'] = brand\n",
    "blue.drop(blue.tail(1).index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We now scrape products after 25th product from page 12.\n",
    "\n",
    "price2 = []\n",
    "prod_url2 = []\n",
    "title2 = []\n",
    "img_url2 = []\n",
    "brand2 = []\n",
    "for j in [12]:\n",
    "    bw.get('https://www.blue-tomato.com/de-AT/products/categories/Snowboard+Shop-00000000/?page={}'.format(j))\n",
    "    for i in np.arange(96)[26:]:          \n",
    "            url = bw.find_element_by_css_selector('#p{} > span.productdesc > a'.format(i)).get_attribute('href')\n",
    "            bw.get(url)\n",
    "            prod_url2.append(url)\n",
    "            try:\n",
    "                price2.append(bw.find_element_by_css_selector('.c-details-box__price-current').text)\n",
    "            except:\n",
    "                price2.append('NAN')\n",
    "            try:\n",
    "                brand2.append(bw.find_element_by_css_selector('.c-details-box__name > span:nth-child(1)').text)\n",
    "            except:\n",
    "                brand2.append('NAN')\n",
    "            try:\n",
    "                title2.append(bw.find_element_by_css_selector('#variantName').text)\n",
    "            except:\n",
    "                title2.append('NAN')\n",
    "            try:\n",
    "                img_url2.append(bw.find_element_by_css_selector('.js-media-box-img').get_attribute('src'))\n",
    "            except:\n",
    "                img_url2.append('NAN')\n",
    "            finally:\n",
    "                bw.get('https://www.blue-tomato.com/de-AT/products/categories/Snowboard+Shop-00000000/?page={}'.format(j))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We create a dataframe for these remaining results for page 12.\n",
    "\n",
    "blue2 = pd.DataFrame(title2, price2)\n",
    "blue2 = blue2.reset_index()\n",
    "blue2.columns = ['Price', 'Name']\n",
    "blue2['Image_URL'] = img_url2\n",
    "blue2['Product_URL'] = prod_url2\n",
    "blue2['Brand'] = brand2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We scrape remaining data from pages 13 to 32.\n",
    "\n",
    "price3 = []\n",
    "prod_url3 = []\n",
    "title3 = []\n",
    "img_url3 = []\n",
    "brand3 = []\n",
    "for j in np.arange(33)[13:]:\n",
    "    bw.get('https://www.blue-tomato.com/de-AT/products/categories/Snowboard+Shop-00000000/?page={}'.format(j))\n",
    "    for i in np.arange(96):          \n",
    "            url = bw.find_element_by_css_selector('#p{} > span.productdesc > a'.format(i)).get_attribute('href')\n",
    "            bw.get(url)\n",
    "            prod_url3.append(url)\n",
    "            try:\n",
    "                price3.append(bw.find_element_by_css_selector('.c-details-box__price-current').text)\n",
    "            except:\n",
    "                price3.append('NAN')\n",
    "            try:\n",
    "                brand3.append(bw.find_element_by_css_selector('.c-details-box__name > span:nth-child(1)').text)\n",
    "            except:\n",
    "                brand3.append('NAN')\n",
    "            try:\n",
    "                title3.append(bw.find_element_by_css_selector('#variantName').text)\n",
    "            except:\n",
    "                title3.append('NAN')\n",
    "            try:\n",
    "                img_url3.append(bw.find_element_by_css_selector('.js-media-box-img').get_attribute('src'))\n",
    "            except:\n",
    "                img_url3.append('NAN')\n",
    "            finally:\n",
    "                bw.get('https://www.blue-tomato.com/de-AT/products/categories/Snowboard+Shop-00000000/?page={}'.format(j))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## And we save the results into a dataframe.\n",
    "\n",
    "blue3 = pd.DataFrame(title3, price3)\n",
    "blue3 = blue3.reset_index()\n",
    "blue3.columns = ['Price', 'Name']\n",
    "blue3['Image_URL'] = img_url3\n",
    "blue3['Product_URL'] = prod_url3\n",
    "blue3['Brand'] = brand3\n",
    "blue3.to_csv('blue3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We finally scrape all the products on page 33\n",
    "\n",
    "price4 = []\n",
    "prod_url4 = []\n",
    "title4 = []\n",
    "img_url4 = []\n",
    "brand4 = []\n",
    "bw.get('https://www.blue-tomato.com/de-AT/products/categories/Snowboard+Shop-00000000/?page={}'.format(33))\n",
    "for i in np.arange(69):\n",
    "    url = bw.find_element_by_css_selector('#p{} > span.productdesc > a'.format(i)).get_attribute('href')\n",
    "    bw.get(url)\n",
    "    prod_url4.append(url)\n",
    "    try:\n",
    "        price4.append(bw.find_element_by_css_selector('.c-details-box__price-current').text)\n",
    "    except:\n",
    "        price4.append('NAN')\n",
    "    try:\n",
    "        brand4.append(bw.find_element_by_css_selector('.c-details-box__name > span:nth-child(1)').text)\n",
    "    except:\n",
    "        brand4.append('NAN')\n",
    "    try:\n",
    "        title4.append(bw.find_element_by_css_selector('#variantName').text)\n",
    "    except:\n",
    "        title4.append('NAN')\n",
    "    try:\n",
    "        img_url4.append(bw.find_element_by_css_selector('.js-media-box-img').get_attribute('src'))\n",
    "    except:\n",
    "        img_url4.append('NAN')\n",
    "    finally:\n",
    "        bw.get('https://www.blue-tomato.com/de-AT/products/categories/Snowboard+Shop-00000000/?page={}'.format(33))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We save these results into a dataframe\n",
    "\n",
    "blue4 = pd.DataFrame(title4, price4)\n",
    "blue4 = blue4.reset_index()\n",
    "blue4.columns = ['Price', 'Name']\n",
    "blue4['Image_URL'] = img_url4\n",
    "blue4['Product_URL'] = prod_url4\n",
    "blue4['Brand'] = brand4\n",
    "blue4.to_csv('blue4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We merge all our results into a single dataframe. \n",
    "\n",
    "data = pd.concat([blue, blue2, blue3, blue4], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We reset our index\n",
    "\n",
    "data.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We now save the results.\n",
    "\n",
    "data.to_csv('Blue_Tomato_Final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We finally quit our browser.\n",
    "\n",
    "bw.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                       1779\n",
       "Price                                                        NAN\n",
       "Name                                                         NAN\n",
       "Image_URL                                                    NAN\n",
       "Product_URL    https://www.blue-tomato.com/de-AT/product/Burt...\n",
       "Brand                                                        NAN\n",
       "Name: 2929, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We have some missing urls so we will use try-except in next step.\n",
    "\n",
    "data.iloc[2929]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let us save all the images from the urls that we found locally. Then I will upload them to the google drive.\n",
    "\n",
    "import urllib\n",
    "for i in np.arange(len(data)):\n",
    "    try:\n",
    "        url = data['Image_URL'][i]\n",
    "        urllib.request.urlretrieve(url, \"image_{}.png\".format(i))\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
